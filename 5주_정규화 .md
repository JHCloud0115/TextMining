### 노이즈와 불용어 제거 

불용어: 의미 없는 특수문자 등과는 별도로, 실제 사용되는 단어이지만 분석에 별 필요가 없는 단어 
  

ex) 영어문장의 경우 - 길이가 짧은 단어들을 삭제함으로 많은 불용어 삭제 가능  
  
  
보통 불용어는 빈도 너무 적거나 혹은 반대로 빈도 너무 많아서 별 필요 없는 단어들도 존재 


```python
from nltk.tokenize import RegexpTokenizer
tokenizer = RegexpTokenizer("[\w']+")
```


```python
text1 = "Sorry, I can't go there."
tokenizer = RegexpTokenizer("[\w']{3,}") 
print(tokenizer.tokenize(text1.lower()))
```

    ['sorry', "can't", 'there']
    

이처럼 보통 길이가 3 미만인 단어들 삭제하는 것이 일반적

특정 단어 지정하여 불용어 사전 만들고 사전 참조하여 불용어 삭제 가능  
NLTK에서는 stopwords라는 라이브러리를 이용


```python
# NLTK의 영어 불용어 사전에 등재된 값을 사용하여 불용어를 제거

from nltk.corpus import stopwords #일반적으로 분석대상이 아닌 단어들
english_stops = set(stopwords.words('english')) #반복이 되지 않도록 set으로 변환

text1 = "Sorry, I couldn't go to movie yesterday."

tokenizer = RegexpTokenizer("[\w']+")
tokens = tokenizer.tokenize(text1.lower()) #word_tokenize로 토큰화

result = [word for word in tokens if word not in english_stops] #stopwords를 제외한 단어들만으로 list를 생성
print(result)
```

    ['sorry', 'go', 'movie', 'yesterday']
    


```python
print(english_stops) #nltk가 제공하는 영어 stopword를 확인
```

    {'yourselves', 'when', 'haven', 'again', 'y', 'them', 'but', 'we', "didn't", 'needn', 'does', 'aren', 'more', 'were', 'because', 'has', 'from', "she's", "won't", 'him', 'where', 'd', 'below', 'isn', 'other', 'hers', 'their', 'hadn', 'ain', 'herself', "shan't", "couldn't", 'o', 'won', 'wouldn', 's', 'themselves', "hasn't", 'ma', 'of', 'same', 'having', 'what', 'nor', "aren't", 'do', 'against', 'into', 'this', 'any', 'those', 'being', 'he', 'how', 'who', "don't", 'whom', 'and', 're', 'these', 'most', 'then', "wasn't", 'as', 'all', 'your', 't', 'is', 'been', 'each', 'itself', 'or', "weren't", 'don', "you're", 'during', 'weren', 'am', 'are', 'll', 'will', 'can', 'didn', "you'll", 'for', 'theirs', 'few', 'm', "hadn't", "wouldn't", 'doing', 'no', 'shan', 'ourselves', 'up', 've', 'himself', 'had', 'hasn', 'my', 'than', 'that', 'i', 'on', 'just', 'under', 'so', 'couldn', 'out', 'wasn', 'an', 'while', 'why', "haven't", 'both', "it's", 'which', 'above', 'mightn', "doesn't", 'be', "mustn't", 'her', 'after', 'shouldn', 'further', 'there', 'by', "needn't", 'the', 'if', 'to', 'our', 'some', 'once', "isn't", 'mustn', 'you', "you've", 'only', 'own', 'at', 'a', 'his', "shouldn't", 'here', 'down', 'with', 'not', 'such', 'over', 'in', 'they', 'should', 'it', 'its', 'until', 'doesn', "you'd", 'was', 'now', "that'll", 'too', 'very', 'ours', 'yourself', 'she', 'have', 'about', 'between', 'before', 'did', 'off', 'through', 'me', "mightn't", "should've", 'yours', 'myself'}
    


```python
#자신만의 stopwords를 만들고 이용
#한글처리에서도 유용하게 사용할 수 있음
my_stopword = ['i', 'go', 'to'] #나만의 stopword를 리스트로 정의
result = [word for word in tokens if word not in my_stopword] 
print(result)
```

    ['sorry', "couldn't", 'movie', 'yesterday']
    

## 정규화

같은 의미 가진 동일한 단어임에도 불구하고, 다른 형태로 쓰여진 단어들을 통일 시켜서 표준 단어로 바꾸는 것을 말한다. 즉, 다양한 변형을 원형의 형태로 통일하는 작업을 정규화라고 함  
* 어간추출 (stemming)
* 표제어 추출(lemmatization)

### 어간추출(stemming)

어형이 변형된 단어로부터 접사 등을 제거하고 그 단어의 어간을 분리해내는 작업
   
영어는 명사가 복수형으로 기술된 것을 단수형으로 바꾸는 작업도 포함하며 아래 두 알고리즘이 유명하다 
* 포터 스테머(Porter Stemmer)
* 랭카스터 스테머(Lancaster Stemmer)

#### 포터 스테머(Porter Stemmer)


```python
from nltk.stem import PorterStemmer
stemmer = PorterStemmer()
print(stemmer.stem('cooking'), stemmer.stem('cookery'), stemmer.stem('cookbooks'))
```

    cook cookeri cookbook
    

항상 올바른 단어가 되지는 않지만, 중요한 점은 포터스테머를 사용하면 모든 단어들이 동일한 규칙에 의해서 변환된다는 것  



```python
from nltk.tokenize import word_tokenize

para = "Hello everyone. It's good to see you. Let's start our text mining class!"
tokens = word_tokenize(para) #토큰화 실행
print(tokens)
result = [stemmer.stem(token) for token in tokens] #모든 토큰에 대해 스테밍 실행
print(result)
```

    ['Hello', 'everyone', '.', 'It', "'s", 'good', 'to', 'see', 'you', '.', 'Let', "'s", 'start', 'our', 'text', 'mining', 'class', '!']
    ['hello', 'everyon', '.', 'It', "'s", 'good', 'to', 'see', 'you', '.', 'let', "'s", 'start', 'our', 'text', 'mine', 'class', '!']
    

#### 랭카스터 스테머(The LancasterStemmer class)


```python
from nltk.stem import LancasterStemmer
stemmer = LancasterStemmer()
print(stemmer.stem('cooking'), stemmer.stem('cookery'), stemmer.stem('cookbooks'))
```

    cook cookery cookbook
    

### 표제어 추출(Lemmatization)

단어의 기본형을 찾는 것이 표제어 추출로 주어진 단어를 기본형으로 변환하는 것을 의미   
예) 작다  --> 어간 - 작  /  사전 - 작다 


```python
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()
print(lemmatizer.lemmatize('cooking'))
print(lemmatizer.lemmatize('cooking', pos='v')) #품사를 지정
print(lemmatizer.lemmatize('cookery'))
print(lemmatizer.lemmatize('cookbooks'))
```

    cooking
    cook
    cookery
    cookbook
    

결과를 보면 첫번째 단어 cooking 사전에 나와있음으로 그대로 둔 것을 확인할 수 있다. 그러나 2번째처럼 품사를 지정해주면,기본형인 cook으로 반환해주는 것을 확인할 수 있다. 



```python
#comparison of lemmatizing and stemming
from nltk.stem import PorterStemmer
stemmer = PorterStemmer()
print('stemming result:', stemmer.stem('believes'))
print('lemmatizing result:', lemmatizer.lemmatize('believes'))
print('lemmatizing result:', lemmatizer.lemmatize('believes', pos='v'))
```

    stemming result: believ
    lemmatizing result: belief
    lemmatizing result: believe
    

## 품사 태깅(Part-of-Speech Tagging)

품사 태깅은 형태소에 대해 품사를 파악하여 부착(tagging)하는 작업
  
형태소 : 의미를 가진 가장 작은 말의 단위 

### NLTK를 이용한 품사 태깅

``nltk.pos_tag()``은 toeknize된 결과에 대해 품사를 태깅하여 (단어, 품사)로 구성된 튜플의 리스트로 품사 태깅 결과를 반환


```python
import nltk
from nltk.tokenize import word_tokenize

tokens = word_tokenize("Hello everyone. It's good to see you. Let's start our text mining class!")
print(nltk.pos_tag(tokens))
```

    [('Hello', 'NNP'), ('everyone', 'NN'), ('.', '.'), ('It', 'PRP'), ("'s", 'VBZ'), ('good', 'JJ'), ('to', 'TO'), ('see', 'VB'), ('you', 'PRP'), ('.', '.'), ('Let', 'VB'), ("'s", 'POS'), ('start', 'VB'), ('our', 'PRP$'), ('text', 'NN'), ('mining', 'NN'), ('class', 'NN'), ('!', '.')]
    


```python
import nltk
nltk.download('tagsets')
# nltk.help.upenn_tagset()를 사용하여 품사 약어의 의미와 설명 확인 가능
nltk.help.upenn_tagset('CC')
```

    CC: conjunction, coordinating
        & 'n and both but either et for less minus neither nor or plus so
        therefore times v. versus vs. whether yet
    

    [nltk_data] Downloading package tagsets to
    [nltk_data]     C:\Users\juhee\AppData\Roaming\nltk_data...
    [nltk_data]   Package tagsets is already up-to-date!
    

**원하는 단어만 추출하는 방법**


```python
my_tag_set = ['NN', 'VB', 'JJ']
my_words = [word for word, tag in nltk.pos_tag(tokens) if tag in my_tag_set]
print(my_words)
```

    ['everyone', 'good', 'see', 'Let', 'start', 'text', 'mining', 'class']
    

**단어에 품사 정보 추가하여 구분**  
  
단어 뒤에 품사 태그 붙여서 사용하는 방법으로 문장의 의미 정확히 파악 가능 


```python
words_with_tag = ['/'.join(item) for item in nltk.pos_tag(tokens)]
print(words_with_tag)
```

    ['Hello/NNP', 'everyone/NN', './.', 'It/PRP', "'s/VBZ", 'good/JJ', 'to/TO', 'see/VB', 'you/PRP', './.', 'Let/VB', "'s/POS", 'start/VB', 'our/PRP$', 'text/NN', 'mining/NN', 'class/NN', '!/.']
    

### 한글 형태소 분석과 품사 태깅


```python
sentence = '''절망의 반대가 희망은 아니다.
어두운 밤하늘에 별이 빛나듯
희망은 절망 속에 싹트는 거지
만약에 우리가 희망함이 적다면
그 누가 세상을 비출어줄까.
정희성, 희망 공부'''
```


```python
tokens = word_tokenize(sentence)
print(tokens)
print(nltk.pos_tag(tokens))
```

    ['절망의', '반대가', '희망은', '아니다', '.', '어두운', '밤하늘에', '별이', '빛나듯', '희망은', '절망', '속에', '싹트는', '거지', '만약에', '우리가', '희망함이', '적다면', '그', '누가', '세상을', '비출어줄까', '.', '정희성', ',', '희망', '공부']
    [('절망의', 'JJ'), ('반대가', 'NNP'), ('희망은', 'NNP'), ('아니다', 'NNP'), ('.', '.'), ('어두운', 'VB'), ('밤하늘에', 'JJ'), ('별이', 'NNP'), ('빛나듯', 'NNP'), ('희망은', 'NNP'), ('절망', 'NNP'), ('속에', 'NNP'), ('싹트는', 'NNP'), ('거지', 'NNP'), ('만약에', 'NNP'), ('우리가', 'NNP'), ('희망함이', 'NNP'), ('적다면', 'NNP'), ('그', 'NNP'), ('누가', 'NNP'), ('세상을', 'NNP'), ('비출어줄까', 'NNP'), ('.', '.'), ('정희성', 'NN'), (',', ','), ('희망', 'NNP'), ('공부', 'NNP')]
    

NLTK 대신 한국어를 제대로 토큰화하고 품사 태깅을 할 수 있는 다른 라이브러리를 써야 한다는 것 확인할 수 있음


```python
from konlpy.tag import Okt
t = Okt()
```

Q. 왜 한글은 형태소 분석은 사전을 가지고 실시?
  
A. 복합명사 등 규칙으로 한글을 나누기는 어려움 
   물론, 한글 분석이 있기는 하지만 수업시간에는 다루지 않음


### KoNLPy의 형태소 분석 및 품사 태깅 기능 사용법

**morphs(phrase)**: 주어진 텍스트를 형태소 단위로 분리한다. 따라서 반환 결과는 형태소의 리스트이다.
  
**nouns(phrase)**: 주어진 텍스트를 형태소 단위로 분리해서 명사만을 반환한다. 즉 반환 결과는 텍스트에 있는 명사의 리스트이다.
  
**pos(phrase)**: 주어진 텍스트를 형태소 단위로 분리하고, 각 형태소에 품사를 부착하여 반환한다. 반환되는 형태는 NLTK와 동일하게 (단어, 품사)로 구성된 튜플의 리스트이다. NLTK와 다른 점은, NLTK에서는 먼저 토큰화를 하고 품사 태깅을 한 것에 비해 KoNLPy는 품사 태깅 함수가 토큰화를 함께 수행한다는 점이다.


```python
print('형태소:', t.morphs(sentence))
print()
print('명사:', t.nouns(sentence))
print()
print('품사 태깅 결과:', t.pos(sentence))
```

    형태소: ['절망', '의', '반대', '가', '희망', '은', '아니다', '.', '\n', '어', '두운', '밤하늘', '에', '별', '이', '빛나듯', '\n', '희망', '은', '절망', '속', '에', '싹트는', '거지', '\n', '만약', '에', '우리', '가', '희망', '함', '이', '적다면', '\n', '그', '누가', '세상', '을', '비출어줄까', '.', '\n', '정희성', ',', '희망', '공부']
    
    명사: ['절망', '반대', '희망', '어', '두운', '밤하늘', '별', '희망', '절망', '속', '거지', '만약', '우리', '희망', '함', '그', '누가', '세상', '정희성', '희망', '공부']
    
    품사 태깅 결과: [('절망', 'Noun'), ('의', 'Josa'), ('반대', 'Noun'), ('가', 'Josa'), ('희망', 'Noun'), ('은', 'Josa'), ('아니다', 'Adjective'), ('.', 'Punctuation'), ('\n', 'Foreign'), ('어', 'Noun'), ('두운', 'Noun'), ('밤하늘', 'Noun'), ('에', 'Josa'), ('별', 'Noun'), ('이', 'Josa'), ('빛나듯', 'Verb'), ('\n', 'Foreign'), ('희망', 'Noun'), ('은', 'Josa'), ('절망', 'Noun'), ('속', 'Noun'), ('에', 'Josa'), ('싹트는', 'Verb'), ('거지', 'Noun'), ('\n', 'Foreign'), ('만약', 'Noun'), ('에', 'Josa'), ('우리', 'Noun'), ('가', 'Josa'), ('희망', 'Noun'), ('함', 'Noun'), ('이', 'Josa'), ('적다면', 'Verb'), ('\n', 'Foreign'), ('그', 'Noun'), ('누가', 'Noun'), ('세상', 'Noun'), ('을', 'Josa'), ('비출어줄까', 'Verb'), ('.', 'Punctuation'), ('\n', 'Foreign'), ('정희성', 'Noun'), (',', 'Punctuation'), ('희망', 'Noun'), ('공부', 'Noun')]
    
