# 그래프와 워드클라우드

텍스트 분석에서 가장 많이 접하게 되는 것은 빈도를 바탕으로 한 그래프와 워드클라우드인데, 하나 혹은 여러 개의 문서에서 가장 많이 사용된 단어 파악하여 단어 빈도를 그래프로 표현한 것이 워드클라우드 

## 단어 빈도 그래프

gutenberg.open()을 이용하면 파일제목(fileid)로 해당 파일의 내용을 읽을 수 있다


```python
import nltk
nltk.download('gutenberg')

from nltk.corpus import gutenberg
file_names = gutenberg.fileids() #파일 제목을 읽어온다.
print(file_names)
```

    ['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']
    

    [nltk_data] Downloading package gutenberg to
    [nltk_data]     C:\Users\juhee\AppData\Roaming\nltk_data...
    [nltk_data]   Package gutenberg is already up-to-date!
    


```python
doc_alice = gutenberg.open('carroll-alice.txt').read()
print('#Num of characters used:', len(doc_alice)) #사용된 문자의 수
print('#Text sample:')
print(doc_alice[:500]) #앞의 500자만 출력
```

    #Num of characters used: 144395
    #Text sample:
    [Alice's Adventures in Wonderland by Lewis Carroll 1865]
    
    CHAPTER I. Down the Rabbit-Hole
    
    Alice was beginning to get very tired of sitting by her sister on the
    bank, and of having nothing to do: once or twice she had peeped into the
    book her sister was reading, but it had no pictures or conversations in
    it, 'and what is the use of a book,' thought Alice 'without pictures or
    conversation?'
    
    So she was considering in her own mind (as well as she could, for the
    hot day made her feel very sleepy an
    


```python
from nltk.tokenize import word_tokenize

tokens_alice = word_tokenize(doc_alice) #토큰화 실행
print('#Num of tokens used:', len(tokens_alice))
print('#Token sample:')
print(tokens_alice[:20])
```

    #Num of tokens used: 33493
    #Token sample:
    ['[', 'Alice', "'s", 'Adventures', 'in', 'Wonderland', 'by', 'Lewis', 'Carroll', '1865', ']', 'CHAPTER', 'I', '.', 'Down', 'the', 'Rabbit-Hole', 'Alice', 'was', 'beginning']
    

포터스터머 이용하여 스테밍하기 


```python
from nltk.stem import PorterStemmer
stemmer = PorterStemmer()

stem_tokens_alice = [stemmer.stem(token) for token in tokens_alice] #모든 토큰에 대해 스테밍 실행
print('#Num of tokens after stemming:', len(stem_tokens_alice))
print('#Token sample:')
print(stem_tokens_alice[:20])
```

    #Num of tokens after stemming: 33493
    #Token sample:
    ['[', 'alic', "'s", 'adventur', 'in', 'wonderland', 'by', 'lewi', 'carrol', '1865', ']', 'chapter', 'I', '.', 'down', 'the', 'rabbit-hol', 'alic', 'wa', 'begin']
    

WordNetLemmatizer를 이용하여 표제어 추출


```python
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()

lem_tokens_alice = [lemmatizer.lemmatize(token) for token in tokens_alice] #모든 토큰에 대해 스테밍 실행
print('#Num of tokens after lemmatization:', len(lem_tokens_alice))
print('#Token sample:')
print(lem_tokens_alice[:20])
```

    #Num of tokens after lemmatization: 33493
    #Token sample:
    ['[', 'Alice', "'s", 'Adventures', 'in', 'Wonderland', 'by', 'Lewis', 'Carroll', '1865', ']', 'CHAPTER', 'I', '.', 'Down', 'the', 'Rabbit-Hole', 'Alice', 'wa', 'beginning']
    

위의 둘 결과를 비교해 보았을 때, 어간추출이든 표제어추출이든 토큰의 수는 변하지 않음


```python
from nltk.tokenize import RegexpTokenizer
tokenizer = RegexpTokenizer("[\w']{3,}") 

reg_tokens_alice = tokenizer.tokenize(doc_alice.lower())
print('#Num of tokens with RegexpTokenizer:', len(reg_tokens_alice))
print('#Token sample:')
print(reg_tokens_alice[:20])
```

    #Num of tokens with RegexpTokenizer: 21616
    #Token sample:
    ["alice's", 'adventures', 'wonderland', 'lewis', 'carroll', '1865', 'chapter', 'down', 'the', 'rabbit', 'hole', 'alice', 'was', 'beginning', 'get', 'very', 'tired', 'sitting', 'her', 'sister']
    

WordTokenizer를 사용한 결과를 보면 '[',']'와 같은 부호 사라져서 토큰의 수 줄어든 것을 확인할 수 있음. 포함과 제외여부는 사용자의 몫. 

이번 실습에선 RegexpTokenizer의 결과로 수행  
  
불용어 제거하고 분석의 목적이 그래프 이용한 시각화임으로 단어 알아보기 쉽도록 스테밍 수행하지 않기로 함


```python
from nltk.corpus import stopwords #일반적으로 분석대상이 아닌 단어들
english_stops = set(stopwords.words('english')) #반복이 되지 않도록 set으로 변환

result_alice = [word for word in reg_tokens_alice if word not in english_stops] #stopwords를 제외한 단어들만으로 list를 생성

print('#Num of tokens after stopword elimination:', len(result_alice))
print('#Token sample:')
print(result_alice[:20])
```

    #Num of tokens after stopword elimination: 12999
    #Token sample:
    ["alice's", 'adventures', 'wonderland', 'lewis', 'carroll', '1865', 'chapter', 'rabbit', 'hole', 'alice', 'beginning', 'get', 'tired', 'sitting', 'sister', 'bank', 'nothing', 'twice', 'peeped', 'book']
    

텍스트 전처리 완료 후 단어별로 빈도 계산하기 


```python
alice_word_count = dict()
for word in result_alice:
    alice_word_count[word] = alice_word_count.get(word, 0) + 1

print('#Num of used words:', len(alice_word_count))

sorted_word_count = sorted(alice_word_count, key=alice_word_count.get, reverse=True)

print("#Top 20 high frequency words:")
for key in sorted_word_count[:20]: #빈도수 상위 20개의 단어를 출력
    print(f'{repr(key)}: {alice_word_count[key]}', end=', ')
```

    #Num of used words: 2687
    #Top 20 high frequency words:
    'said': 462, 'alice': 385, 'little': 128, 'one': 98, 'know': 88, 'like': 85, 'went': 83, 'would': 78, 'could': 77, 'thought': 74, 'time': 71, 'queen': 68, 'see': 67, 'king': 61, 'began': 58, 'turtle': 57, "'and": 56, 'way': 56, 'mock': 56, 'quite': 55, 

'would', 'could', 'and'와 같은 단어는 중요하지 않아보임으로 품사 태깅 이용해서 의미 있는 것으로 생각되는 명사, 형용사,동사만 추출하기


```python
my_tag_set = ['NN', 'VB', 'VBD', 'JJ'] #불러올 품사 설정
my_words = [word for word, tag in nltk.pos_tag(result_alice) if tag in my_tag_set]
#print(my_words)

alice_word_count = dict()
for word in my_words:
    alice_word_count[word] = alice_word_count.get(word, 0) + 1

print('#Num of used words:', len(alice_word_count))

sorted_word_count = sorted(alice_word_count, key=alice_word_count.get, reverse=True)

print("#Top 20 high frequency words:")
for key in sorted_word_count[:20]: #빈도수 상위 20개의 단어를 출력
    print(f'{repr(key)}: {alice_word_count[key]}', end=', ')
```

    #Num of used words: 1726
    #Top 20 high frequency words:
    'said': 462, 'alice': 293, 'little': 124, 'went': 83, 'time': 71, 'queen': 66, 'began': 58, 'way': 56, 'turtle': 56, 'mock': 55, 'thought': 54, 'thing': 49, 'voice': 48, 'head': 46, 'gryphon': 45, 'got': 44, 'rabbit': 42, 'looked': 42, 'see': 42, 'came': 40, 

그래프 이용해 시각화하기 


```python
import matplotlib.pyplot as plt
%matplotlib inline

w = [alice_word_count[key] for key in sorted_word_count] #정렬된 단어 리스트에 대해 빈도수를 가져와서 리스트 생성
plt.plot(w)
plt.show()
```


![png](output_21_0.png)


확인해본 결과 어떤 단어가 많이 사용되었는지 확인할 수 없음.  
그래프의 모양은 빈도수에 따라 정렬된 단어의 순위와 빈도수가 극단적으로 반비례하는 것을 보여줌  
이를 통해 주로 사용하는 단어만 쓰는 경향을 알 수 있음
